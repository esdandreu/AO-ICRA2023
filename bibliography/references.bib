@article{Horiko2020,
  title    = {Experimental study on wheel-soil interaction mechanics using in-wheel sensor and particle image velocimetry part II. Analysis and modeling of shear stress of lightweight wheeled vehicle},
  journal  = {Journal of Terramechanics},
  volume   = {91},
  pages    = {243-256},
  year     = {2020},
  author   = {Shota Horiko and Genya Ishigami},
  keywords = {Wheel-soil interaction, Shear stress, In-wheel sensor, Particle image velocimetry},
  abstract = {Wheeled vehicle mobility on loose sand is highly subject to shear deformation of sand around the wheel because the shear stress generates traction force of the wheel. The main contribution of this paper is to improve a shear stress model for a lightweight wheeled vehicle on dry sand. This work exploits two experimental approaches, an in-wheel sensor and a particle image velocimetry that precisely measure the shear stress and shear deformation generated at the interaction boundary. Further, the paper improves a shear stress model. The model proposed in this paper considers a force chain generated inside the granular media, boundary friction between the wheel surface and sand, and velocity dependency of the friction. The proposed model is experimentally validated, and its usefulness is confirmed through numerical simulation of the wheel traction force. The simulation result confirmed that the proposed model calculated the traction force with an accuracy about 70%, whereas the conventional one overestimated the force, and its accuracy was 13% at the best.}
}
@article{SENATORE2014,
  title    = {Analysis of stress distributions under lightweight wheeled vehicles},
  journal  = {Journal of Terramechanics},
  volume   = {51},
  pages    = {1-17},
  year     = {2014},
  author   = {C. Senatore and K. Iagnemma},
  keywords = {Wheel model, Shear strength, Stress sensor, Granular particle image velocimetry, Terrain shearing failure, Wheel dynamics, Off-road vehicle performance}
}
@inproceedings{Slip2009,
  author    = {Ding, Liang and Gao, Haibo and Deng, Zongquan and Yoshida, Kazuya and Nagatani, Keiji},
  booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  title     = {Slip ratio for lugged wheel of planetary rover in deformable soil: definition and estimation},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {3343-3348},
}
@inproceedings{Cooke1993ModellingAP,
  title     = {Modelling auditory processing and organisation},
  author    = {{Martin Cooke}},
  booktitle = {Distinguished dissertations in computer science},
  year      = {1993}
}
@inproceedings{marchegiani2018a,
  year      = {2018},
  journal   = {19th Annual Conference Towards Autonomous Robotics Systems (TAROS 2018)},
  publisher = {Institute of Electrical and Electronics Engineers},
  title     = {Learning to listen to your ego-(motion): Metric motion estimation from auditory signals},
  author    = {Marchegiani, L and Newman, P}
}
@article{GTF1998,
  title   = {Implementing a gammatone filterbank},
  journal = {Annex C of the SVOS Final Report: Part A: The Auditory Filterbank},
  volume  = {1},
  pages   = {1-5},
  year    = {1988},
  author  = {Holdsworth, J. and Nimmo-Smith, I. and Patterson, R. and Rice, P.}
}
@mimisc{EfficientGTF,
  author = {{Ning Ma}},
  title  = {An Efficient Implementation of Gammatone Filters},
}
@article{GLASBERG1990103,
  title    = {Derivation of auditory filter shapes from notched-noise data},
  journal  = {Hearing Research},
  volume   = {47},
  number   = {1},
  pages    = {103-138},
  year     = {1990},
  author   = {Brian R Glasberg and Brian C.J Moore},
  keywords = {Frequency selectivity, Auditory filter, Masking, Excitation pattern, Power-spectrum model},
  abstract = {A well established method for estimating the shape of the auditory filter is based on the measurement of the threshold of a sinusoidal signal in a notched-noise masker, as a function of notch width. To measure the asymmetry of the filter, the notch has to be placed both symmetrically and asymmetrically about the signal frequency. In previous work several simplifying assumptions and approximations were made in deriving auditory filter shapes from the data. In this paper we describe modifications to the fitting procedure which allow more accurate derivations. These include: 1) taking into account changes in filter bandwidth with centre frequency when allowing for the effects of off-frequency listening; 2) correcting for the non-flat frequency response of the earphone; 3) correcting for the transmission characteristics of the outer and middle ear; 4) limiting the amount by which the centre frequency of the filter can shift in order to maximise the signal-to-masker ratio. In many cases, these modifications result in only small changes to the derived filter shape. However, at very high and very low centre frequencies and for hearing-impaired subjects the differences can be substantial. It is also shown that filter shapes derived from data where the notch is always placed symmetrically about the signal frequency can be seriously in error when the underlying filter is markedly asymmetric. New formulae are suggested describing the variation of the auditory filter with frequency and level. The implications of the results for the calculation of excitation patterns are discussed and a modified procedure is proposed. The appendix lists FORTRAN computer programs for deriving auditory filter shapes from notched-noise data and for calculating excitation patterns. The first program can readily be modified so as to derive auditory filter shapes from data obtained with other types of maskers, such as rippled noise.}
}
@article{CorrelogramMa2007,
  author  = {Ma, Ning and Green, Phil and Barker, Jon and Coy, Andre},
  year    = {2007},
  pages   = {874-891},
  title   = {Exploiting correlogram structure for robust speech recognition with multiple speech sources},
  volume  = {49},
  journal = {Speech Communication},
}
@article{FukushimaCNN,
  author  = {Fukushima, K.},
  journal = {Trans. IECE},
  number  = {J62},
  title   = {Neural network model for a mechanism of pattern recognition unaffected by shift in position - Noecognitron},
  volume  = {A-(10)},
  year    = {1979},
  pages   = {658-665}
}
@misc{batchnorm2015,
  author    = {Ioffe, Sergey and Szegedy, Christian},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{ordclass2006,
  author    = {Li, Ling and Lin, Hsuan-tien},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
  pages     = {},
  publisher = {MIT Press},
  title     = {Ordinal Regression by Extended Binary Classification},
  volume    = {19},
  year      = {2006}
}
@incollection{NEURIPS2019_9015,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
} 
@article{TL2016,
  author  = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  year    = {2016},
  pages   = {9},
  title   = {A survey of transfer learning},
  volume  = {3},
  journal = {Journal of Big Data},
}
@misc{Measuring2019,
  author    = {Prokhorov, David and Zhukov, Dmitry and Barinova, Olga and Vorontsova, Anna and Konushin, Anton},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Measuring robustness of Visual SLAM},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@online{WebsterRobot,
  author  = {Merriam-Webster.com},
  title   = {robot},
  year    = {2011},
}
@incollection{Chen09,
  author    = {X.Q. Chen and Y.Q. Chen and J.G. Chase},
  title     = {Mobiles Robots - Past Present and Future},
  booktitle = {Mobile Robots},
  publisher = {IntechOpen},
  address   = {Rijeka},
  year      = {2009},
  editor    = {XiaoQi Chen and Y.Q. Chen and J.G. Chase},
  chapter   = {1},
}
@incollection{Sanchez09,
  author    = {Gary Boucher and Luz Maria Sanchez},
  title     = {Mobile Wheeled Robot with Step Climbing Capabilities},
  booktitle = {Mobile Robots},
  publisher = {IntechOpen},
  address   = {Rijeka},
  year      = {2009},
  editor    = {XiaoQi Chen and Y.Q. Chen and J.G. Chase},
  chapter   = {3},
}
@inbook{Localization2016,
  author    = {Huang, Shoudong and Dissanayake, Gamini},
  publisher = {John Wiley and Sons, Ltd},
  title     = {Robot Localization: An Introduction},
  booktitle = {Wiley Encyclopedia of Electrical and Electronics Engineering},
  pages     = {1-10},
  year      = {2016},
  keywords  = {Kalman filter, least squares, localization, Markov localization, mobile robots, particle filter}
}
@inproceedings{Masayoshi2006,
  author    = {Wada, Masayoshi},
  booktitle = {2006 IEEE International Conference on Robotics and Biomimetics},
  title     = {Studies on 4WD Mobile Robots Climbing Up a Step},
  year      = {2006},
  volume    = {},
  number    = {},
  pages     = {1529-1534},
}
@article{Amar2007,
  author  = {Faiz Ben Amar and Guillermo Andrade and Christophe Grand and Frédéric Plumet},
  journal = {International Journal of Factory Automation, Robotics and Soft Computing},
  title   = {Towards an advanced mobility of wheeled robots on diﬀicult terrain},
  volume  = {hal-03135894},
  pages   = {40-45},
  year    = {2007}
}
@article{OdometrySurvey,
  author  = {Mohamed, Sherif A. S. and Haghbayan, Mohammad-Hashem and Westerlund, Tomi and Heikkonen, Jukka and Tenhunen, Hannu and Plosila, Juha},
  journal = {IEEE Access},
  title   = {A Survey on Odometry for Autonomous Navigation Systems},
  year    = {2019},
  volume  = {7},
  pages   = {97466-97486},
}
@article{ScaramuzzaTutorial,
  author  = {Scaramuzza, Davide and Fraundorfer, Friedrich},
  journal = {IEEE Robotics and Automation Magazine},
  title   = {Visual Odometry [Tutorial]},
  year    = {2011},
  volume  = {18},
  number  = {4},
  pages   = {80-92},
}
@misc{Valente2019,
  author    = {Valente, Michelle and Joly, Cyril and de La Fortelle, Arnaud},
  keywords  = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Deep Sensor Fusion for Real-Time Odometry Estimation},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{Long2021,
  year      = {2021},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = {6},
  number    = {2},
  pages     = {3703-3710},
  author    = {Ran Long and Christian Rauch and Tianwei Zhang and Vladimir Ivan and Sethu Vijayakumar},
  title     = {{RigidFusion}: Robot Localisation and Mapping in Environments With Large Dynamic Rigid Objects},
  journal   = {{IEEE} Robotics and Automation Letters}
}
@inproceedings{Vargas2021,
  author    = {Vargas, Elizabeth and Scona, Raluca and Willners, Jonatan Scharff and Luczynski, Tomasz and Cao, Yu and Wang, Sen and Petillot, Yvan R.},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Robust Underwater Visual SLAM Fusing Acoustic Sensing},
  year      = {2021},
  pages     = {2140-2146},
}
@article{Ojeda2006,
  author  = {Ojeda, L. and Cruz, D. and Reina, G. and Borenstein, J.},
  journal = {IEEE Transactions on Robotics},
  title   = {Current-Based Slippage Detection and Odometry Correction for Mobile Robots and Planetary Rovers},
  year    = {2006},
  volume  = {22},
  number  = {2},
  pages   = {366-378},
}
@misc{DFVO,
  author    = {Zhan, Huangying and Weerasekera, Chamara Saroj and Bian, Jia-Wang and Garg, Ravi and Reid, Ian},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {DF-VO: What Should Be Learnt for Visual Odometry?},
  publisher = {arXiv},
  year      = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@article{VoiceForklift,
  author   = {Walter, Matthew R. and Antone, Matthew and Chuangsuwanich, Ekapol and Correa, Andrew and Davis, Randall and Fletcher, Luke and Frazzoli, Emilio and Friedman, Yuli and Glass, James and How, Jonathan P. and Jeon, Jeong hwan and Karaman, Sertac and Luders, Brandon and Roy, Nicholas and Tellex, Stefanie and Teller, Seth},
  title    = {A Situationally Aware Voice-commandable Robotic Forklift Working Alongside People in Unstructured Outdoor Environments},
  journal  = {Journal of Field Robotics},
  volume   = {32},
  number   = {4},
  pages    = {590-628},
  abstract = {One long-standing challenge in robotics is the realization of mobile autonomous robots able to operate safely in human workplaces, and be accepted by the human occupants. We describe the development of a multiton robotic forklift intended to operate alongside people and vehicles, handling palletized materials within existing, active outdoor storage facilities. The system has four novel characteristics. The first is a multimodal interface that allows users to efficiently convey task-level commands to the robot using a combination of pen-based gestures and natural language speech. These tasks include the manipulation, transport, and placement of palletized cargo within dynamic, human-occupied warehouses. The second is the robot's ability to learn the visual identity of an object from a single user-provided example and use the learned model to reliably and persistently detect objects despite significant spatial and temporal excursions. The third is a reliance on local sensing that allows the robot to handle variable palletized cargo and navigate within dynamic, minimally prepared environments without a global positioning system. The fourth concerns the robot's operation in close proximity to people, including its human supervisor, pedestrians who may cross or block its path, moving vehicles, and forklift operators who may climb inside the robot and operate it manually. This is made possible by interaction mechanisms that facilitate safe, effective operation around people. This paper provides a comprehensive description of the system's architecture and implementation, indicating how real-world operational requirements motivated key design choices. We offer qualitative and quantitative analyses of the robot operating in real settings and discuss the lessons learned from our effort.},
  year     = {2015}
}
@online{SDG,
  author  = {United Nations},
  title   = {Sustainable Development Goals},
}
@inproceedings{SnakeSound,
  author    = {Bando, Yoshiaki and Suhara, Hiroki and Tanaka, Motoyasu and Kamegawa, Tetsushi and Itoyama, Katsutoshi and Yoshii, Kazuyoshi and Matsuno, Fumitoshi and Okuno, Hiroshi G.},
  booktitle = {2016 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)},
  title     = {Sound-based online localization for an in-pipe snake robot},
  year      = {2016},
  pages     = {207-213},
}
@inproceedings{HowDoISoundLike,
  author    = {Pico, Antonio and Schillaci, Guido and Hafner, Verena V. and Lara, Bruno},
  booktitle = {2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)},
  title     = {How do I sound like? forward models for robot ego-noise prediction},
  year      = {2016},
  pages     = {246-251},
}
@article{AcousticSLAM,
  author     = {Evers, Christine and Naylor, Patrick A.},
  title      = {Acoustic SLAM},
  year       = {2018},
  issue_date = {September 2018},
  publisher  = {IEEE Press},
  volume     = {26},
  number     = {9},
  abstract   = {An algorithm is presented that enables devices equipped with microphones, such as robots, to move within their environment in order to explore, adapt to, and interact with sound sources of interest. Acoustic scene mapping creates a three-dimensional 3D representation of the positional information of sound sources across time and space. In practice, positional source information is only provided by Direction-of-Arrival DoA estimates of the source directions; the source-sensor range is typically difficult to obtain. DoA estimates are also adversely affected by reverberation, noise, and interference, leading to errors in source location estimation and consequent false DoA estimates. Moreover, many acoustic sources, such as human talkers, are not continuously active, such that periods of inactivity lead to missing DoA estimates. Withal, the DoA estimates are specified relative to the observer's sensor location and orientation. Accurate positional information about the observer therefore is crucial. This paper proposes Acoustic Simultaneous Localization and Mapping aSLAM, which uses acoustic signals to simultaneously map the 3D positions of multiple sound sources while passively localizing the observer within the scene map. The performance of aSLAM is analyzed and evaluated using a series of realistic simulations. Results are presented to show the impact of the observer motion and sound source localization accuracy.},
  journal    = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  pages      = {1484-1498},
  numpages   = {15}
}
@inproceedings{SoundSourceMapping,
  author    = {Su, Daobilige and Nakamura, Keisuke and Nakadai, Kazuhiro and Miro, Jaime Valls},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title     = {Robust sound source mapping using three-layered selective audio rays for mobile robots},
  year      = {2016},
  pages     = {2771-2777},
}
@article{Allen2012,
  author  = {Brian F. Allen and Flavien Picon and Sébastien Dalibard and Nadia Magnenat-Thalmann and Daniel Thalmann},
  journal = {3DTV-CON 2012},
  title   = {Localizing a mobile robot with intrinsic noise},
  volume  = {hal-00732764},
  year    = {2012}
}
@inproceedings{Gautam2014,
  author    = {Narang, Gautam and Nakamura, Keisuke and Nakadai, Kazuhiro},
  booktitle = {2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  title     = {Auditory-aware navigation for mobile robots based on reflection-robust sound source localization and visual SLAM},
  year      = {2014},
  pages     = {4021-4026},
}
@misc{AcousticFusion,
  author    = {Zhang, Tianwei and Zhang, Huayan and Li, Xiaofei and Chen, Junfeng and Lam, Tin Lun and Vijayakumar, Sethu},
  keywords  = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {AcousticFusion: Fusing Sound Source Localization to Visual SLAM in Dynamic Environments},
  publisher = {arXiv},
  year      = {2021},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}
@inbook{Valada2018,
  author    = {Valada, Abhinav and Spinello, Luciano and Burgard, Wolfram},
  editor    = {Bicchi, Antonio and Burgard, Wolfram},
  title     = {Deep Feature Learning for Acoustics-Based Terrain Classification},
  booktitle = {Robotics Research: Volume 2},
  year      = {2018},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {21--37},
  abstract  = {In order for robots to efficiently navigate in real-world environments, they need to be able to classify and characterize terrain for safe navigation. The majority of techniques for terrain classification is predominantly based on using visual features. However, as vision-based approaches are severely affected by appearance variations and occlusions, relying solely on them incapacitates the ability to function robustly in all conditions. In this paper, we propose an approach that uses sound from vehicle-terrain interactions for terrain classification. We present a new convolutional neural network architecture that learns deep features from spectrograms of extensive audio signals, gathered from interactions with various indoor and outdoor terrains. Using exhaustive experiments, we demonstrate that our network significantly outperforms classification approaches using traditional audio features by achieving state of the art performance. Additional experiments reveal the robustness of the network in situations corrupted with varying amounts of white Gaussian noise and that fine-tuning with noise-augmented samples significantly boosts the classification rate. Furthermore, we demonstrate that our network performs exceptionally well even with samples recorded with a low-quality mobile phone microphone that adds substantial amount of environmental noise.},
}
@article{DeepTerrain,
  author    = {Zürn, Jannik and Burgard, Wolfram and Valada, Abhinav},
  keywords  = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Self-Supervised Visual Terrain Classification from Unsupervised Acoustic Feature Learning},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{Kurobe2021,
  author  = {Kurobe, Akiyoshi and Nakajima, Yoshikatsu and Kitani, Kris and Saito, Hideo},
  journal = {IEEE Access},
  title   = {Audio-Visual Self-Supervised Terrain Type Recognition for Ground Mobile Platforms},
  year    = {2021},
  volume  = {9},
  pages   = {29970-29979},
}
@inproceedings{Ishikawa2021,
  author    = {Ishikawa, Reina and Hachiuma, Ryo and Kurobe, Akiyoshi and Saito, Hideo},
  booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
  title     = {Single-modal Incremental Terrain Clustering from Self-Supervised Audio-Visual Feature Learning},
  year      = {2021},
  pages     = {9399-9406},
}
@misc{vaswani2017attention,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{li2020transformer,
  title         = {Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry},
  author        = {Xiangyu Li and Yonghong Hou and Pichao Wang and Zhimin Gao and Mingliang Xu and Wanqing Li},
  year          = {2020},
  eprint        = {2101.02143},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{kim2022squeezeformer,
  title         = {Squeezeformer: An Efficient Transformer for Automatic Speech Recognition},
  author        = {Sehoon Kim and Amir Gholami and Albert Shaw and Nicholas Lee and Karttikeya Mangalam and Jitendra Malik and Michael W. Mahoney and Kurt Keutzer},
  year          = {2022},
  eprint        = {2206.00888},
  archiveprefix = {arXiv},
  primaryclass  = {eess.AS}
}
